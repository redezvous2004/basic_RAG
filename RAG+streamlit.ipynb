{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers\n",
        "!pip install -q bitsandbytes # Optimize for supporting to calculate on GPU\n",
        "!pip install -q accelerate # Speed up training model\n",
        "!pip install -q langchain\n",
        "!pip install -q langchainhub\n",
        "!pip install -q langchain-chroma # A vector database for LLM\n",
        "!pip install -q langchain_experimental\n",
        "!pip install -q langchain-community\n",
        "!pip install -q langchain_huggingface\n",
        "!pip install -q python-dotenv==1.1.0\n",
        "!pip install -q pypdf\n",
        "!pip install -q streamlit"
      ],
      "metadata": {
        "id": "GzrQ5vwi9n2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef6vkeXK9eJn"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import torch\n",
        "import re\n",
        "import tempfile\n",
        "import os\n",
        "import streamlit as st\n",
        "\n",
        "from transformers import BitsAndBytesConfig\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_huggingface.llms import HuggingFacePipeline\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain import hub\n",
        "\n",
        "@st.cache_resource\n",
        "def load_embeddings(model_name: str):\n",
        "    return HuggingFaceEmbeddings(model_name=model_name)\n",
        "\n",
        "@st.cache_resource\n",
        "def load_llm(model_name: str, config):\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=config,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        low_cpu_mem_usage=True\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model_pipeline = pipeline(\n",
        "        task=\"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=512,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    return HuggingFacePipeline(pipeline=model_pipeline)\n",
        "\n",
        "def remove_invalid_surrogates(text):\n",
        "  return re.sub(r'[\\ud800-\\udfff]', '', text)\n",
        "\n",
        "def process_pdf(uploaded_file, session_state):\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmpf:\n",
        "        tmpf.write(uploaded_file.getvalue())\n",
        "        tmpf_path = tmpf.name\n",
        "\n",
        "    loader = PyPDFLoader(tmpf_path)\n",
        "    documents = loader.load()\n",
        "\n",
        "    semanctic_splitter = SemanticChunker(\n",
        "        embeddings=session_state.embeddings,\n",
        "        buffer_size=1,\n",
        "        breakpoint_threshold_type=\"percentile\",\n",
        "        breakpoint_threshold_amount=95,\n",
        "        min_chunk_size=500,\n",
        "        add_start_index=True\n",
        "    )\n",
        "\n",
        "    docs = semanctic_splitter.split_documents(documents=documents)\n",
        "    for doc in docs:\n",
        "      doc.page_content = remove_invalid_surrogates(doc.page_content)\n",
        "\n",
        "    vec_db = Chroma.from_documents(\n",
        "        documents=docs,\n",
        "        embedding=session_state.embeddings\n",
        "    )\n",
        "    retriever = vec_db.as_retriever()\n",
        "\n",
        "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "    def format_docs(docs):\n",
        "        return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "    rag_chain = (\n",
        "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "        | prompt\n",
        "        | session_state.llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    os.unlink(tmpf_path)  # Clean up the temporary file\n",
        "    return rag_chain, len(docs)\n",
        "\n",
        "nf4_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, # weights is round to 4-bit number\n",
        "    bnb_4bit_quant_type=\"nf4\", # A technic for quantizing model to get smaller\n",
        "    bnb_4bit_use_double_quant=True, # Double quantize\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16 # brain float16\n",
        ")\n",
        "\n",
        "\n",
        "# Make sure model are loaded each interaction\n",
        "if \"rag_chain\" not in st.session_state: # Rag chain build from pdf\n",
        "    st.session_state.rag_chain = None\n",
        "if \"models_loaded\" not in st.session_state:\n",
        "    st.session_state.models_loaded = False\n",
        "if \"embeddings\" not in st.session_state:\n",
        "    st.session_state.embeddings = None\n",
        "if \"llm\" not in st.session_state:\n",
        "    st.session_state.llm = None\n",
        "\n",
        "st.set_page_config(page_title=\"RAG Assistant\", layout=\"wide\")\n",
        "st.title(\"RAG PDF Assistant\")\n",
        "\n",
        "st.markdown(\"\"\"\n",
        "**AI allows you to directly ask questions abd get answers from the contecnt of PDF documents in Vietnamese**\n",
        "\n",
        "**Simple use:**\n",
        "1. **Upload PDF:** Choose a PDF file to upload and click \"Process PDF\"\n",
        "2. **Question:** Type your question about the content of file that you've just uploaded\n",
        "\"\"\")\n",
        "\n",
        "if not st.session_state.models_loaded:\n",
        "    st.info(\"Downloading models...\")\n",
        "    st.session_state.embeddings = load_embeddings(\"bkai-foundation-models/vietnamese-bi-encoder\")\n",
        "    st.session_state.llm = load_llm(\"lmsys/vicuna-7b-v1.5\", nf4_config)\n",
        "    st.session_state.models_loaded = True\n",
        "    st.success(\"Models loaded successfully!\")\n",
        "    st.rerun()\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Upload PDF\", type=[\"pdf\"])\n",
        "if uploaded_file and st.button(\"Process PDF\"):\n",
        "    with st.spinner(\"Processing PDF...\"):\n",
        "        st.session_state.rag_chain, num_chunks = process_pdf(uploaded_file, st.session_state)\n",
        "        st.success(f\"PDF processed successfully! Number of chunks: {num_chunks}\")\n",
        "\n",
        "if st.session_state.rag_chain:\n",
        "    question = st.text_input(\"Ask a question:\")\n",
        "    if question:\n",
        "        with st.spinner(\"Answering...\"):\n",
        "            output = st.session_state.rag_chain.invoke(question)\n",
        "            answer = output.split(\"Answer: \")[1].strip() if \"Answer:\" in output else output\n",
        "            st.write(\"Answer: \", answer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "print(\"Password/Enpoint IP for localtunnel is:\",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))"
      ],
      "metadata": {
        "id": "1Uj6uoXx-G8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "id": "afIWgeIj-HhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "2G1-MFLJ-IqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "6Ng4yHgZ-JyH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}